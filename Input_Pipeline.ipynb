{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fb37a4b7-a510-4c43-9cbb-4540460db697",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tensorflow\n",
    "!pip install -q tensorflow_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "097104e6-1103-4afa-8619-c9f824c0fe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import os\n",
    "\n",
    "PATH = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1be38527-841e-405c-be81-bbb6b63cbd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_ranking as tfr\n",
    "import tensorflow as tf\n",
    "from tensorflow_serving.apis import input_pb2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eec5900-63e0-4792-8723-95b38b8cd26e",
   "metadata": {},
   "source": [
    "## Import Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6da9c646-6efc-4578-adaa-1a026e748b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAL</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>ABBV</th>\n",
       "      <th>ABC</th>\n",
       "      <th>ABT</th>\n",
       "      <th>ADP</th>\n",
       "      <th>AIG</th>\n",
       "      <th>AMD</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>AXP</th>\n",
       "      <th>...</th>\n",
       "      <th>UAL</th>\n",
       "      <th>UNH</th>\n",
       "      <th>UPS</th>\n",
       "      <th>USB</th>\n",
       "      <th>V</th>\n",
       "      <th>VZ</th>\n",
       "      <th>WFC</th>\n",
       "      <th>WMT</th>\n",
       "      <th>WY</th>\n",
       "      <th>XOM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-01-08</th>\n",
       "      <td>54.733580</td>\n",
       "      <td>48.380133</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65.547120</td>\n",
       "      <td>27.375300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.688020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>34.364357</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.591780</td>\n",
       "      <td>82.466700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.274380</td>\n",
       "      <td>29.868883</td>\n",
       "      <td>33.745920</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.079986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>47.648250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.639543</td>\n",
       "      <td>54.500400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.078583</td>\n",
       "      <td>48.24400</td>\n",
       "      <td>64.369029</td>\n",
       "      <td>53.963917</td>\n",
       "      <td>...</td>\n",
       "      <td>35.314671</td>\n",
       "      <td>63.975633</td>\n",
       "      <td>48.553633</td>\n",
       "      <td>66.298814</td>\n",
       "      <td>34.397120</td>\n",
       "      <td>65.478033</td>\n",
       "      <td>58.864167</td>\n",
       "      <td>43.275140</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.451929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-22</th>\n",
       "      <td>46.533920</td>\n",
       "      <td>54.592783</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46.323871</td>\n",
       "      <td>53.083133</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.472780</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.238817</td>\n",
       "      <td>47.179850</td>\n",
       "      <td>...</td>\n",
       "      <td>37.417229</td>\n",
       "      <td>39.147600</td>\n",
       "      <td>61.799500</td>\n",
       "      <td>54.161680</td>\n",
       "      <td>NaN</td>\n",
       "      <td>63.555560</td>\n",
       "      <td>45.811833</td>\n",
       "      <td>48.762014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.629000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-29</th>\n",
       "      <td>61.440733</td>\n",
       "      <td>59.969157</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.877529</td>\n",
       "      <td>37.299460</td>\n",
       "      <td>70.67464</td>\n",
       "      <td>45.605017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.332650</td>\n",
       "      <td>32.367800</td>\n",
       "      <td>...</td>\n",
       "      <td>33.507200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.766960</td>\n",
       "      <td>57.530733</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49.758420</td>\n",
       "      <td>30.632383</td>\n",
       "      <td>53.260180</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.641543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-02-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>45.843986</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.872640</td>\n",
       "      <td>39.915000</td>\n",
       "      <td>51.85292</td>\n",
       "      <td>46.007020</td>\n",
       "      <td>55.69945</td>\n",
       "      <td>44.701514</td>\n",
       "      <td>67.632367</td>\n",
       "      <td>...</td>\n",
       "      <td>37.830000</td>\n",
       "      <td>69.142433</td>\n",
       "      <td>39.716300</td>\n",
       "      <td>73.979850</td>\n",
       "      <td>31.954333</td>\n",
       "      <td>60.893186</td>\n",
       "      <td>59.940980</td>\n",
       "      <td>42.406833</td>\n",
       "      <td>15.7469</td>\n",
       "      <td>28.347643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  AAL       AAPL  ABBV        ABC        ABT       ADP  \\\n",
       "Date                                                                     \n",
       "2012-01-08  54.733580  48.380133   NaN  65.547120  27.375300       NaN   \n",
       "2012-01-15        NaN  47.648250   NaN  52.639543  54.500400       NaN   \n",
       "2012-01-22  46.533920  54.592783   NaN  46.323871  53.083133       NaN   \n",
       "2012-01-29  61.440733  59.969157   NaN  58.877529  37.299460  70.67464   \n",
       "2012-02-05        NaN  45.843986   NaN  58.872640  39.915000  51.85292   \n",
       "\n",
       "                  AIG       AMD       AMZN        AXP  ...        UAL  \\\n",
       "Date                                                   ...              \n",
       "2012-01-08        NaN       NaN  40.688020        NaN  ...  34.364357   \n",
       "2012-01-15  34.078583  48.24400  64.369029  53.963917  ...  35.314671   \n",
       "2012-01-22  38.472780       NaN  51.238817  47.179850  ...  37.417229   \n",
       "2012-01-29  45.605017       NaN  52.332650  32.367800  ...  33.507200   \n",
       "2012-02-05  46.007020  55.69945  44.701514  67.632367  ...  37.830000   \n",
       "\n",
       "                  UNH        UPS        USB          V         VZ        WFC  \\\n",
       "Date                                                                           \n",
       "2012-01-08        NaN  39.591780  82.466700        NaN  37.274380  29.868883   \n",
       "2012-01-15  63.975633  48.553633  66.298814  34.397120  65.478033  58.864167   \n",
       "2012-01-22  39.147600  61.799500  54.161680        NaN  63.555560  45.811833   \n",
       "2012-01-29        NaN  47.766960  57.530733        NaN  49.758420  30.632383   \n",
       "2012-02-05  69.142433  39.716300  73.979850  31.954333  60.893186  59.940980   \n",
       "\n",
       "                  WMT       WY        XOM  \n",
       "Date                                       \n",
       "2012-01-08  33.745920      NaN  18.079986  \n",
       "2012-01-15  43.275140      NaN  14.451929  \n",
       "2012-01-22  48.762014      NaN  27.629000  \n",
       "2012-01-29  53.260180      NaN  28.641543  \n",
       "2012-02-05  42.406833  15.7469  28.347643  \n",
       "\n",
       "[5 rows x 108 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables_names = ['big_log_ret','big_RCV'] #, 'big_RVT', 'big_positivePartscr', 'big_negativePartscr', 'big_splogscr', 'big_linscr']\n",
    "tables_dict = {}\n",
    "for name in tables_names:\n",
    "    table = pd.read_csv(os.path.join(PATH, 'Tables', name+'.csv'))\n",
    "    table['Date'] = pd.to_datetime(table['Date']).dt.date\n",
    "    table.set_index('Date', inplace=True)\n",
    "    tables_dict[name] = table\n",
    "\n",
    "w_sent.to_csv(os.path.join(PATH,'Tables','big_{}.csv'.format(w_t_name))) #Store in csv format in the 'Tables' folder\n",
    "tables_dict['big_RCV'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7eba07-f2c0-49fe-87a8-f57804e4c39e",
   "metadata": {},
   "source": [
    "## Merging Tables - Building the query-stock final table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f47e7f6b-00b3-41b6-be1a-7528539882ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>big_log_ret</th>\n",
       "      <th>big_RCV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-01-08</td>\n",
       "      <td>AAL</td>\n",
       "      <td>0.063980</td>\n",
       "      <td>54.733580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-01-08</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.006150</td>\n",
       "      <td>48.380133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-01-08</td>\n",
       "      <td>ABC</td>\n",
       "      <td>-0.020684</td>\n",
       "      <td>65.547120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-01-08</td>\n",
       "      <td>ABT</td>\n",
       "      <td>0.000864</td>\n",
       "      <td>27.375300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-01-08</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>-0.023212</td>\n",
       "      <td>40.688020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35351</th>\n",
       "      <td>2021-11-28</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>-0.063878</td>\n",
       "      <td>31.423967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35352</th>\n",
       "      <td>2021-11-28</td>\n",
       "      <td>TSN</td>\n",
       "      <td>0.021006</td>\n",
       "      <td>45.611180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35353</th>\n",
       "      <td>2021-11-28</td>\n",
       "      <td>UAL</td>\n",
       "      <td>-0.039827</td>\n",
       "      <td>14.679940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35354</th>\n",
       "      <td>2021-11-28</td>\n",
       "      <td>UPS</td>\n",
       "      <td>-0.022512</td>\n",
       "      <td>46.677617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35355</th>\n",
       "      <td>2021-11-28</td>\n",
       "      <td>USB</td>\n",
       "      <td>-0.033782</td>\n",
       "      <td>68.229333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35356 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Query Ticker  big_log_ret    big_RCV\n",
       "0      2012-01-08    AAL     0.063980  54.733580\n",
       "1      2012-01-08   AAPL    -0.006150  48.380133\n",
       "2      2012-01-08    ABC    -0.020684  65.547120\n",
       "3      2012-01-08    ABT     0.000864  27.375300\n",
       "4      2012-01-08   AMZN    -0.023212  40.688020\n",
       "...           ...    ...          ...        ...\n",
       "35351  2021-11-28   TSLA    -0.063878  31.423967\n",
       "35352  2021-11-28    TSN     0.021006  45.611180\n",
       "35353  2021-11-28    UAL    -0.039827  14.679940\n",
       "35354  2021-11-28    UPS    -0.022512  46.677617\n",
       "35355  2021-11-28    USB    -0.033782  68.229333\n",
       "\n",
       "[35356 rows x 4 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_table = tables_dict['big_log_ret'].stack() #Stack DF to a Series so that is grouped by Date and then by Ticker\n",
    "final_table.index.names=('Query','Ticker') #Rename 'Date' to 'Query'\n",
    "final_table = final_table.reset_index()\n",
    "final_table.rename(columns={0: 'big_log_ret'}, inplace=True)\n",
    "resting_tables_dict = dict(tables_dict) #Make a copy of the dictionary\n",
    "del resting_tables_dict['big_log_ret'] #Remove the log_ret table\n",
    "\n",
    "for i, t_name in enumerate(resting_tables_dict):\n",
    "    table = tables_dict[t_name].stack()\n",
    "    table.index.names=('Query','Ticker')\n",
    "    table = table.reset_index()\n",
    "    table.rename(columns={0: t_name}, inplace=True)\n",
    "    final_table = pd.merge(final_table, table, on=['Query','Ticker'])\n",
    "\n",
    "final_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eb957c-0e1e-4295-861b-05c2ade20008",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Aqui tenemos q añadir un filtro para eliminar aquellas filas q tengan un NaN y tambien si queremos hacer algun preprocessing previo\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "89a7b9ac-34c4-4e95-a428-ae5dd35122c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'big_log_ret': float_list {\n",
       "   value: -0.00615044217556715\n",
       " },\n",
       " 'big_RCV': float_list {\n",
       "   value: 48.38013458251953\n",
       " }}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fl_feature(value_list):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value_list]))\n",
    "\n",
    "\n",
    "{\n",
    "  f\"{final_table.columns[f_n+2]}\":fl_feature((f_v))\n",
    "      for (f_n, f_v) in enumerate(np.array(final_table)[1,2:])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8ca0e56b-6d92-4733-bbac-844b4da3c95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Protobuffers are extensible structures suitable for storing data in a serialized format, either locally or in a distributed manner.\n",
    "TF ranking has a couple of pre-defined protobufs such as ELWC which make it easier to integrate and formalize data ingestion into\n",
    "the ranking pipeline.\n",
    "\n",
    "Protocol buffers and the tf.data API is a set of utilities that provide a mechanism to read and store data for efficient loading\n",
    "and preprocessing in a way that's fast and scalable.\n",
    "'''\n",
    "\n",
    "# Class I wrote to organize code resposible for parsing our dataframe data and \n",
    "# creating compressed TF-Records in ELWC protobuf format so that they used by \n",
    "# most rankers in TF-Ranking and be compatible with future rankers or new methods\n",
    "\n",
    "class DFToELWCProto():\n",
    "    \"\"\" Class to parse Dataframe ranking data in ELWC proto TFRecords\"\"\"\n",
    "    \n",
    "    def __init__(self, dir:str=\".\", use_compression:bool=False):\n",
    "        assert isinstance(dir,str)\n",
    "        if not os.path.isdir(dir):\n",
    "            os.mkdir(dir)\n",
    "        self.input_path = dir\n",
    "        assert isinstance(use_compression,bool)\n",
    "        self.compress = use_compression\n",
    "        if self.compress:\n",
    "            self.compress_type = 'GZIP'\n",
    "        else:\n",
    "            self.compress_type = None\n",
    "\n",
    "\n",
    "    # Helper functions (see also https://www.tensorflow.org/tutorials/load_data/tf_records)\n",
    "    def _bytes_feature(self,value_list):\n",
    "        \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "        if isinstance(value_list, type(tf.constant(0))):\n",
    "            value_list = value_list.numpy()\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value_list]))\n",
    "\n",
    "    def _float_feature(self,value_list):\n",
    "        \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "        return tf.train.Feature(float_list=tf.train.FloatList(value=[value_list]))\n",
    "\n",
    "    def _int64_feature(self,value_list):\n",
    "        \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value_list]))\n",
    "\n",
    "\n",
    "    def read_and_print_topn_tfrecord(self, target_filename, num_of_examples_to_read):\n",
    "        filenames = [target_filename]\n",
    "        tf_record_dataset = tf.data.TFRecordDataset(filenames,\n",
    "                                                    compression_type=self.compress_type)\n",
    "\n",
    "        for raw_record in tf_record_dataset.take(num_of_examples_to_read):\n",
    "            example_list_with_context = input_pb2.ExampleListWithContext()\n",
    "            example_list_with_context.ParseFromString(raw_record.numpy())\n",
    "            print(example_list_with_context)\n",
    "\n",
    "    def df_to_TFrecord(self, df, file_name):\n",
    "\n",
    "        \"\"\" \n",
    "        for reading and converting directly from arrays in memory\n",
    "\n",
    "        \"\"\"    \n",
    "        file_name = os.path.basename(file_name)\n",
    "\n",
    "        if self.compress:\n",
    "            print('Using GZIP compression for writing ELWC TFRecord Dataset')\n",
    "            opts = tf.io.TFRecordOptions(compression_type = self.compress_type)\n",
    "            file_name = f\"{file_name}.gzipped_tfrecord\"\n",
    "        else:\n",
    "            file_name = f\"{file_name}.tfrecord\"\n",
    "        save_path = f\"{self.input_path}/{file_name}\"\n",
    "\n",
    "        with tf.io.TFRecordWriter(f\"{save_path}\", options=opts) as writer:\n",
    "            \n",
    "            input_array = np.array(df)\n",
    "            col_names = df.columns\n",
    "\n",
    "            ELWC = input_pb2.ExampleListWithContext()\n",
    "            prev_qid = None\n",
    "\n",
    "            for i in range(input_array.shape[0]):\n",
    "\n",
    "                qid, doc, r, features = str(input_array[i,0]), input_array[i,1], input_array[i,2], input_array[i,3:]\n",
    "\n",
    "                example_proto_dict = {\n",
    "                              f\"{col_names[f_n+3]}\":self._float_feature((f_v))\n",
    "                                  for (f_n, f_v) in enumerate(features)\n",
    "                          }\n",
    "                example_proto_dict['rel'] = self._int64_feature(int(r))\n",
    "                #example_proto_dict['doc_name'] = self._bytes_feature(str(doc))\n",
    "\n",
    "                example_proto = tf.train.Example(\n",
    "                            features=tf.train.Features(feature=example_proto_dict))\n",
    "                if qid != prev_qid:\n",
    "                    if prev_qid is not None:\n",
    "                        writer.write(ELWC.SerializeToString())\n",
    "                    prev_qid = qid\n",
    "                    ELWC = input_pb2.ExampleListWithContext()\n",
    "                    ELWC.examples.append(example_proto)\n",
    "                else:\n",
    "                    ELWC.examples.append(example_proto)\n",
    "\n",
    "            # final write for the last query grp\n",
    "            writer.write(ELWC.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "39115f8d-8766-4382-b8e7-0dd11c588064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GZIP compression for writing ELWC TFRecord Dataset\n"
     ]
    }
   ],
   "source": [
    "ELWC_converter = DFToELWCProto(dir =\"./Tables/LTR-tfrecords\",\n",
    "                              use_compression=True)\n",
    "ELWC_converter.df_to_TFrecord(final_table,\"LTR_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2ac05d-a5db-4fb3-b1d1-f5849b5a781d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the paths to files containing training and test instances.\n",
    "_DATA_PATH = f\"./Tables/LTR-tfrecords.gzip_tfrecord\"\n",
    "\n",
    "# The maximum number of documents per query in the dataset.\n",
    "# Document lists are padded or truncated to this size.\n",
    "_LIST_SIZE = 200\n",
    "\n",
    "# The document relevance label.\n",
    "_LABEL_FEATURE_NAME = \"rel\"\n",
    "_NUM_FEATURES = 136\n",
    "\n",
    "# Padding labels are set negative so that the corresponding examples can be\n",
    "# ignored in loss and metrics.\n",
    "_PADDING_LABEL = -1\n",
    "\n",
    "# Learning rate for optimizer.\n",
    "_LEARNING_RATE = 0.05\n",
    "\n",
    "# Parameters to the scoring function.\n",
    "_BATCH_SIZE = 128\n",
    "_DROPOUT_RATE = 0.5\n",
    "\n",
    "# Location of model directory and number of training steps.\n",
    "_MODEL_DIR = f\"./Models/model_{dt.datetime.now().strftime('%m-%d-%Y_%H-%M-%S')}\"\n",
    "\n",
    "# setting as shell env for tensorboard stuff\n",
    "os.environ[\"models_dir\"] = _MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b0140a07-dc74-4f06-9563-e923866f7d55",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (Temp/ipykernel_72848/3741735708.py, line 38)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\ericb\\AppData\\Local\\Temp/ipykernel_72848/3741735708.py\"\u001b[1;36m, line \u001b[1;32m38\u001b[0m\n\u001b[1;33m    '''computes elementwise log_e(|x|)*sign(x) '''\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def create_dataset_from_tfrecords(input_path:str,\n",
    "                                  batch_sz:int,\n",
    "                                  shuffle:bool = True,\n",
    "                                  num_epochs:int = None,\n",
    "                                  data_format:str = \"ELWC\",\n",
    "                                  compression_type:str = ''):\n",
    "\n",
    "    context_feature_columns, example_feature_columns = create_feature_columns()\n",
    "\n",
    "\n",
    "    context_feature_spec = tf.feature_column.make_parse_example_spec(\n",
    "      context_feature_columns.values())\n",
    "    label_column = tf.feature_column.numeric_column(\n",
    "      _LABEL_FEATURE_NAME, dtype=tf.int64, default_value=_PADDING_LABEL)\n",
    "    example_feature_spec = tf.feature_column.make_parse_example_spec(\n",
    "      list(example_feature_columns.values()) + [label_column])\n",
    "\n",
    "    _reader_arg_list = []\n",
    "    if compression_type:\n",
    "        assert compression_type in [\"\", \"GZIP\",\"ZLIB\"]\n",
    "        _reader_arg_list = [compression_type]\n",
    "\n",
    "\n",
    "    dataset = tfr.data.build_ranking_dataset(\n",
    "      file_pattern=input_path,\n",
    "      data_format=tfr.data.ELWC,\n",
    "      batch_size=batch_sz,\n",
    "      list_size=_LIST_SIZE,\n",
    "      context_feature_spec=context_feature_spec,\n",
    "      example_feature_spec=example_feature_spec,\n",
    "      reader=tf.data.TFRecordDataset,\n",
    "      reader_args= _reader_arg_list,\n",
    "      shuffle=shuffle,\n",
    "      num_epochs=num_epochs,\n",
    "      )\n",
    "\n",
    "    def _log1p_transform(features):\n",
    "    '''computes elementwise log_e(|x|)*sign(x) '''\n",
    "    transformed_feats = {\n",
    "        f:tf.math.multiply(\n",
    "            tf.math.log1p(\n",
    "                tf.math.abs(features[f])\n",
    "                ),\n",
    "            tf.math.sign(features[f])\n",
    "            )\n",
    "        for f in features}\n",
    "    return transformed_feats\n",
    "\n",
    "    def _split_label_and_transform_features(features):\n",
    "    label = tf.squeeze(features.pop(_LABEL_FEATURE_NAME), axis=2)\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    features = _log1p_transform(features)\n",
    "\n",
    "    return features, label\n",
    "\n",
    "    dataset = dataset.map(_split_label_and_transform_features)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971744d9-bd4d-4768-96c0-b062da29d479",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
