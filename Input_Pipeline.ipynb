{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fb37a4b7-a510-4c43-9cbb-4540460db697",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tensorflow\n",
    "!pip install -q tensorflow_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "097104e6-1103-4afa-8619-c9f824c0fe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import os\n",
    "\n",
    "PATH = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1be38527-841e-405c-be81-bbb6b63cbd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_ranking as tfr\n",
    "import tensorflow as tf\n",
    "from tensorflow_serving.apis import input_pb2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eec5900-63e0-4792-8723-95b38b8cd26e",
   "metadata": {},
   "source": [
    "## Import Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6da9c646-6efc-4578-adaa-1a026e748b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAL</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>ABBV</th>\n",
       "      <th>ABC</th>\n",
       "      <th>ABT</th>\n",
       "      <th>ADP</th>\n",
       "      <th>AIG</th>\n",
       "      <th>AMD</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>AXP</th>\n",
       "      <th>...</th>\n",
       "      <th>UAL</th>\n",
       "      <th>UNH</th>\n",
       "      <th>UPS</th>\n",
       "      <th>USB</th>\n",
       "      <th>V</th>\n",
       "      <th>VZ</th>\n",
       "      <th>WFC</th>\n",
       "      <th>WMT</th>\n",
       "      <th>WY</th>\n",
       "      <th>XOM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-01-08</th>\n",
       "      <td>8.4760</td>\n",
       "      <td>13.162167</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.404000</td>\n",
       "      <td>-24.607200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.370000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>12.493714</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-9.7618</td>\n",
       "      <td>8.849167</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.031400</td>\n",
       "      <td>7.316500</td>\n",
       "      <td>33.678600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.957143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>11.965000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-4.760143</td>\n",
       "      <td>-3.706000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-11.584333</td>\n",
       "      <td>16.834600</td>\n",
       "      <td>-18.815571</td>\n",
       "      <td>-10.443167</td>\n",
       "      <td>...</td>\n",
       "      <td>12.935143</td>\n",
       "      <td>11.435500</td>\n",
       "      <td>-1.2800</td>\n",
       "      <td>-17.410857</td>\n",
       "      <td>-33.085800</td>\n",
       "      <td>-10.427667</td>\n",
       "      <td>21.078333</td>\n",
       "      <td>15.656600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-18.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-22</th>\n",
       "      <td>9.9768</td>\n",
       "      <td>-10.776667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-17.240429</td>\n",
       "      <td>-22.233833</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-10.898400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.001167</td>\n",
       "      <td>0.860833</td>\n",
       "      <td>...</td>\n",
       "      <td>2.058714</td>\n",
       "      <td>17.216800</td>\n",
       "      <td>-8.6162</td>\n",
       "      <td>-24.026800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-31.776000</td>\n",
       "      <td>-0.758500</td>\n",
       "      <td>-14.170714</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-19.522714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-29</th>\n",
       "      <td>-20.9400</td>\n",
       "      <td>-9.334000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.740000</td>\n",
       "      <td>-8.165800</td>\n",
       "      <td>-42.6148</td>\n",
       "      <td>-22.226000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-7.894167</td>\n",
       "      <td>-33.329167</td>\n",
       "      <td>...</td>\n",
       "      <td>9.101000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.8662</td>\n",
       "      <td>-52.884000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.221000</td>\n",
       "      <td>-20.201500</td>\n",
       "      <td>0.036800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.625571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-02-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-33.438857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.313000</td>\n",
       "      <td>-30.020400</td>\n",
       "      <td>-39.8710</td>\n",
       "      <td>-18.959800</td>\n",
       "      <td>-30.327667</td>\n",
       "      <td>-3.668714</td>\n",
       "      <td>-20.110167</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.529429</td>\n",
       "      <td>-36.037833</td>\n",
       "      <td>4.4648</td>\n",
       "      <td>-42.687167</td>\n",
       "      <td>14.249667</td>\n",
       "      <td>-47.767143</td>\n",
       "      <td>-7.948200</td>\n",
       "      <td>-45.108167</td>\n",
       "      <td>-28.786</td>\n",
       "      <td>8.348429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                AAL       AAPL  ABBV        ABC        ABT      ADP  \\\n",
       "Date                                                                  \n",
       "2012-01-08   8.4760  13.162167   NaN   0.404000 -24.607200      NaN   \n",
       "2012-01-15      NaN  11.965000   NaN  -4.760143  -3.706000      NaN   \n",
       "2012-01-22   9.9768 -10.776667   NaN -17.240429 -22.233833      NaN   \n",
       "2012-01-29 -20.9400  -9.334000   NaN  11.740000  -8.165800 -42.6148   \n",
       "2012-02-05      NaN -33.438857   NaN  -2.313000 -30.020400 -39.8710   \n",
       "\n",
       "                  AIG        AMD       AMZN        AXP  ...        UAL  \\\n",
       "Date                                                    ...              \n",
       "2012-01-08        NaN        NaN  -3.370000        NaN  ...  12.493714   \n",
       "2012-01-15 -11.584333  16.834600 -18.815571 -10.443167  ...  12.935143   \n",
       "2012-01-22 -10.898400        NaN  15.001167   0.860833  ...   2.058714   \n",
       "2012-01-29 -22.226000        NaN  -7.894167 -33.329167  ...   9.101000   \n",
       "2012-02-05 -18.959800 -30.327667  -3.668714 -20.110167  ...  -5.529429   \n",
       "\n",
       "                  UNH      UPS        USB          V         VZ        WFC  \\\n",
       "Date                                                                         \n",
       "2012-01-08        NaN  -9.7618   8.849167        NaN   8.031400   7.316500   \n",
       "2012-01-15  11.435500  -1.2800 -17.410857 -33.085800 -10.427667  21.078333   \n",
       "2012-01-22  17.216800  -8.6162 -24.026800        NaN -31.776000  -0.758500   \n",
       "2012-01-29        NaN  22.8662 -52.884000        NaN   8.221000 -20.201500   \n",
       "2012-02-05 -36.037833   4.4648 -42.687167  14.249667 -47.767143  -7.948200   \n",
       "\n",
       "                  WMT      WY        XOM  \n",
       "Date                                      \n",
       "2012-01-08  33.678600     NaN   1.957143  \n",
       "2012-01-15  15.656600     NaN -18.020000  \n",
       "2012-01-22 -14.170714     NaN -19.522714  \n",
       "2012-01-29   0.036800     NaN   6.625571  \n",
       "2012-02-05 -45.108167 -28.786   8.348429  \n",
       "\n",
       "[5 rows x 108 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables_names = ['big_log_ret','big_RCV'] #, 'big_RVT', 'big_positivePartscr', 'big_negativePartscr', 'big_splogscr', 'big_linscr']\n",
    "tables_dict = {}\n",
    "for name in tables_names:\n",
    "    table = pd.read_csv(os.path.join(PATH, 'Tables', name+'.csv'))\n",
    "    table['Date'] = pd.to_datetime(table['Date']).dt.date\n",
    "    table.set_index('Date', inplace=True)\n",
    "    tables_dict[name] = table\n",
    "\n",
    "tables_dict['big_RCV'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7eba07-f2c0-49fe-87a8-f57804e4c39e",
   "metadata": {},
   "source": [
    "## Merging Tables - Building the query-stock final table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f47e7f6b-00b3-41b6-be1a-7528539882ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>big_log_ret</th>\n",
       "      <th>big_RCV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-01-08</td>\n",
       "      <td>AAL</td>\n",
       "      <td>0.063980</td>\n",
       "      <td>54.733580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-01-08</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.006150</td>\n",
       "      <td>48.380133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-01-08</td>\n",
       "      <td>ABC</td>\n",
       "      <td>-0.020684</td>\n",
       "      <td>65.547120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-01-08</td>\n",
       "      <td>ABT</td>\n",
       "      <td>0.000864</td>\n",
       "      <td>27.375300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-01-08</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>-0.023212</td>\n",
       "      <td>40.688020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35351</th>\n",
       "      <td>2021-11-28</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>-0.063878</td>\n",
       "      <td>31.423967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35352</th>\n",
       "      <td>2021-11-28</td>\n",
       "      <td>TSN</td>\n",
       "      <td>0.021006</td>\n",
       "      <td>45.611180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35353</th>\n",
       "      <td>2021-11-28</td>\n",
       "      <td>UAL</td>\n",
       "      <td>-0.039827</td>\n",
       "      <td>14.679940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35354</th>\n",
       "      <td>2021-11-28</td>\n",
       "      <td>UPS</td>\n",
       "      <td>-0.022512</td>\n",
       "      <td>46.677617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35355</th>\n",
       "      <td>2021-11-28</td>\n",
       "      <td>USB</td>\n",
       "      <td>-0.033782</td>\n",
       "      <td>68.229333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35356 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Query Ticker  big_log_ret    big_RCV\n",
       "0      2012-01-08    AAL     0.063980  54.733580\n",
       "1      2012-01-08   AAPL    -0.006150  48.380133\n",
       "2      2012-01-08    ABC    -0.020684  65.547120\n",
       "3      2012-01-08    ABT     0.000864  27.375300\n",
       "4      2012-01-08   AMZN    -0.023212  40.688020\n",
       "...           ...    ...          ...        ...\n",
       "35351  2021-11-28   TSLA    -0.063878  31.423967\n",
       "35352  2021-11-28    TSN     0.021006  45.611180\n",
       "35353  2021-11-28    UAL    -0.039827  14.679940\n",
       "35354  2021-11-28    UPS    -0.022512  46.677617\n",
       "35355  2021-11-28    USB    -0.033782  68.229333\n",
       "\n",
       "[35356 rows x 4 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_table = tables_dict['big_log_ret'].stack() #Stack DF to a Series so that is grouped by Date and then by Ticker\n",
    "final_table.index.names=('Query','Ticker') #Rename 'Date' to 'Query'\n",
    "final_table = final_table.reset_index()\n",
    "final_table.rename(columns={0: 'big_log_ret'}, inplace=True)\n",
    "resting_tables_dict = dict(tables_dict) #Make a copy of the dictionary\n",
    "del resting_tables_dict['big_log_ret'] #Remove the log_ret table\n",
    "\n",
    "for i, t_name in enumerate(resting_tables_dict):\n",
    "    table = tables_dict[t_name].stack()\n",
    "    table.index.names=('Query','Ticker')\n",
    "    table = table.reset_index()\n",
    "    table.rename(columns={0: t_name}, inplace=True)\n",
    "    final_table = pd.merge(final_table, table, on=['Query','Ticker'])\n",
    "\n",
    "final_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eb957c-0e1e-4295-861b-05c2ade20008",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Aqui tenemos q añadir un filtro para eliminar aquellas filas q tengan un NaN y tambien si queremos hacer algun preprocessing previo\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8ca0e56b-6d92-4733-bbac-844b4da3c95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Protobuffers are extensible structures suitable for storing data in a serialized format, either locally or in a distributed manner.\n",
    "TF ranking has a couple of pre-defined protobufs such as ELWC which make it easier to integrate and formalize data ingestion into\n",
    "the ranking pipeline.\n",
    "\n",
    "Protocol buffers and the tf.data API is a set of utilities that provide a mechanism to read and store data for efficient loading\n",
    "and preprocessing in a way that's fast and scalable.\n",
    "'''\n",
    "\n",
    "# Class I wrote to organize code resposible for parsing our dataframe data and \n",
    "# creating compressed TF-Records in ELWC protobuf format so that they used by \n",
    "# most rankers in TF-Ranking and be compatible with future rankers or new methods\n",
    "\n",
    "class DFToELWCProto():\n",
    "    \"\"\" Class to parse Dataframe ranking data in ELWC proto TFRecords\"\"\"\n",
    "    \n",
    "    def __init__(self, dir:str=\".\", use_compression:bool=False):\n",
    "        assert isinstance(dir,str)\n",
    "        if not os.path.isdir(dir):\n",
    "            os.mkdir(dir)\n",
    "        self.input_path = dir\n",
    "        assert isinstance(use_compression,bool)\n",
    "        self.compress = use_compression\n",
    "        if self.compress:\n",
    "            self.compress_type = 'GZIP'\n",
    "        else:\n",
    "            self.compress_type = None\n",
    "\n",
    "\n",
    "    # Helper functions (see also https://www.tensorflow.org/tutorials/load_data/tf_records)\n",
    "    def _bytes_feature(self,value_list):\n",
    "        \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "        if isinstance(value_list, type(tf.constant(0))):\n",
    "            value_list = value_list.numpy()\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value_list]))\n",
    "\n",
    "    def _float_feature(self,value_list):\n",
    "        \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "        return tf.train.Feature(float_list=tf.train.FloatList(value=[value_list]))\n",
    "\n",
    "    def _int64_feature(self,value_list):\n",
    "        \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value_list]))\n",
    "\n",
    "\n",
    "    def read_and_print_topn_tfrecord(self, target_filename, num_of_examples_to_read):\n",
    "        filenames = [target_filename]\n",
    "        tf_record_dataset = tf.data.TFRecordDataset(filenames,\n",
    "                                                    compression_type=self.compress_type)\n",
    "\n",
    "        for raw_record in tf_record_dataset.take(num_of_examples_to_read):\n",
    "            example_list_with_context = input_pb2.ExampleListWithContext()\n",
    "            example_list_with_context.ParseFromString(raw_record.numpy())\n",
    "            print(example_list_with_context)\n",
    "\n",
    "    def df_to_TFrecord(self, df, file_name):\n",
    "\n",
    "        \"\"\" \n",
    "        for reading and converting directly from arrays in memory\n",
    "\n",
    "        \"\"\"    \n",
    "        file_name = os.path.basename(file_name)\n",
    "\n",
    "        if self.compress:\n",
    "            print('Using GZIP compression for writing ELWC TFRecord Dataset')\n",
    "            opts = tf.io.TFRecordOptions(compression_type = self.compress_type)\n",
    "            file_name = f\"{file_name}.gzipped_tfrecord\"\n",
    "        else:\n",
    "            file_name = f\"{file_name}.tfrecord\"\n",
    "        save_path = f\"{self.input_path}/{file_name}\"\n",
    "\n",
    "        with tf.io.TFRecordWriter(f\"{save_path}\", options=opts) as writer:\n",
    "            \n",
    "            input_array = np.array(df)\n",
    "            col_names = df.columns\n",
    "\n",
    "            ELWC = input_pb2.ExampleListWithContext()\n",
    "            prev_qid = None\n",
    "\n",
    "            for i in range(input_array.shape[0]):\n",
    "\n",
    "                qid, doc, r, features = str(input_array[i,0]), input_array[i,1], input_array[i,2], input_array[i,3:]\n",
    "\n",
    "                example_proto_dict = {\n",
    "                              f\"{col_names[f_n+3]}\":self._float_feature((f_v))\n",
    "                                  for (f_n, f_v) in enumerate(features)\n",
    "                          }\n",
    "                example_proto_dict['rel'] = self._int64_feature(int(r))\n",
    "                #example_proto_dict['doc_name'] = self._bytes_feature(str(doc))\n",
    "\n",
    "                example_proto = tf.train.Example(\n",
    "                            features=tf.train.Features(feature=example_proto_dict))\n",
    "                if qid != prev_qid:\n",
    "                    if prev_qid is not None:\n",
    "                        writer.write(ELWC.SerializeToString())\n",
    "                    prev_qid = qid\n",
    "                    ELWC = input_pb2.ExampleListWithContext()\n",
    "                    ELWC.examples.append(example_proto)\n",
    "                else:\n",
    "                    ELWC.examples.append(example_proto)\n",
    "\n",
    "            # final write for the last query grp\n",
    "            writer.write(ELWC.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "39115f8d-8766-4382-b8e7-0dd11c588064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GZIP compression for writing ELWC TFRecord Dataset\n"
     ]
    }
   ],
   "source": [
    "ELWC_converter = DFToELWCProto(dir =\"./Tables/LTR-tfrecords\",\n",
    "                              use_compression=True)\n",
    "ELWC_converter.df_to_TFrecord(final_table,\"LTR_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2ac05d-a5db-4fb3-b1d1-f5849b5a781d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the paths to files containing training and test instances.\n",
    "_DATA_PATH = f\"./Tables/LTR-tfrecords.gzip_tfrecord\"\n",
    "\n",
    "# The maximum number of documents per query in the dataset.\n",
    "# Document lists are padded or truncated to this size.\n",
    "_LIST_SIZE = 200\n",
    "\n",
    "# The document relevance label.\n",
    "_LABEL_FEATURE_NAME = \"rel\"\n",
    "_NUM_FEATURES = 136\n",
    "\n",
    "# Padding labels are set negative so that the corresponding examples can be\n",
    "# ignored in loss and metrics.\n",
    "_PADDING_LABEL = -1\n",
    "\n",
    "# Learning rate for optimizer.\n",
    "_LEARNING_RATE = 0.05\n",
    "\n",
    "# Parameters to the scoring function.\n",
    "_BATCH_SIZE = 128\n",
    "_DROPOUT_RATE = 0.5\n",
    "\n",
    "# Location of model directory and number of training steps.\n",
    "_MODEL_DIR = f\"./Models/model_{dt.datetime.now().strftime('%m-%d-%Y_%H-%M-%S')}\"\n",
    "\n",
    "# setting as shell env for tensorboard stuff\n",
    "os.environ[\"models_dir\"] = _MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b0140a07-dc74-4f06-9563-e923866f7d55",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (Temp/ipykernel_72848/3741735708.py, line 38)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\ericb\\AppData\\Local\\Temp/ipykernel_72848/3741735708.py\"\u001b[1;36m, line \u001b[1;32m38\u001b[0m\n\u001b[1;33m    '''computes elementwise log_e(|x|)*sign(x) '''\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def create_dataset_from_tfrecords(input_path:str,\n",
    "                                  batch_sz:int,\n",
    "                                  shuffle:bool = True,\n",
    "                                  num_epochs:int = None,\n",
    "                                  data_format:str = \"ELWC\",\n",
    "                                  compression_type:str = ''):\n",
    "\n",
    "    context_feature_columns, example_feature_columns = create_feature_columns()\n",
    "\n",
    "\n",
    "    context_feature_spec = tf.feature_column.make_parse_example_spec(\n",
    "      context_feature_columns.values())\n",
    "    label_column = tf.feature_column.numeric_column(\n",
    "      _LABEL_FEATURE_NAME, dtype=tf.int64, default_value=_PADDING_LABEL)\n",
    "    example_feature_spec = tf.feature_column.make_parse_example_spec(\n",
    "      list(example_feature_columns.values()) + [label_column])\n",
    "\n",
    "    _reader_arg_list = []\n",
    "    if compression_type:\n",
    "        assert compression_type in [\"\", \"GZIP\",\"ZLIB\"]\n",
    "        _reader_arg_list = [compression_type]\n",
    "\n",
    "\n",
    "    dataset = tfr.data.build_ranking_dataset(\n",
    "      file_pattern=input_path,\n",
    "      data_format=tfr.data.ELWC,\n",
    "      batch_size=batch_sz,\n",
    "      list_size=_LIST_SIZE,\n",
    "      context_feature_spec=context_feature_spec,\n",
    "      example_feature_spec=example_feature_spec,\n",
    "      reader=tf.data.TFRecordDataset,\n",
    "      reader_args= _reader_arg_list,\n",
    "      shuffle=shuffle,\n",
    "      num_epochs=num_epochs,\n",
    "      )\n",
    "\n",
    "    def _log1p_transform(features):\n",
    "    '''computes elementwise log_e(|x|)*sign(x) '''\n",
    "    transformed_feats = {\n",
    "        f:tf.math.multiply(\n",
    "            tf.math.log1p(\n",
    "                tf.math.abs(features[f])\n",
    "                ),\n",
    "            tf.math.sign(features[f])\n",
    "            )\n",
    "        for f in features}\n",
    "    return transformed_feats\n",
    "\n",
    "    def _split_label_and_transform_features(features):\n",
    "    label = tf.squeeze(features.pop(_LABEL_FEATURE_NAME), axis=2)\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    features = _log1p_transform(features)\n",
    "\n",
    "    return features, label\n",
    "\n",
    "    dataset = dataset.map(_split_label_and_transform_features)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971744d9-bd4d-4768-96c0-b062da29d479",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
