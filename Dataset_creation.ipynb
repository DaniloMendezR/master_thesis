{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "rGgPRppP7kri",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18783,
     "status": "ok",
     "timestamp": 1652184018038,
     "user": {
      "displayName": "Danilo Méndez",
      "userId": "17401835669463718921"
     },
     "user_tz": -120
    },
    "id": "rGgPRppP7kri",
    "outputId": "f970fecb-e472-4ebf-fa75-440b1c5fc151"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb37a4b7-a510-4c43-9cbb-4540460db697",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9758,
     "status": "ok",
     "timestamp": 1652184027790,
     "user": {
      "displayName": "Danilo Méndez",
      "userId": "17401835669463718921"
     },
     "user_tz": -120
    },
    "id": "fb37a4b7-a510-4c43-9cbb-4540460db697",
    "outputId": "5822a4a5-5492-49dc-d6c6-d6ae38a70d2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 462 kB 7.5 MB/s \n",
      "\u001b[K     |████████████████████████████████| 141 kB 7.4 MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q tensorflow\n",
    "!pip install -q tensorflow_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "097104e6-1103-4afa-8619-c9f824c0fe06",
   "metadata": {
    "executionInfo": {
     "elapsed": 378,
     "status": "ok",
     "timestamp": 1652184028162,
     "user": {
      "displayName": "Danilo Méndez",
      "userId": "17401835669463718921"
     },
     "user_tz": -120
    },
    "id": "097104e6-1103-4afa-8619-c9f824c0fe06"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import os\n",
    "\n",
    "PATH = os.getcwd()\n",
    "#PATH = '/content/drive/Shareddrives/Master Tesis/Tesis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1be38527-841e-405c-be81-bbb6b63cbd4f",
   "metadata": {
    "executionInfo": {
     "elapsed": 2795,
     "status": "ok",
     "timestamp": 1652184030952,
     "user": {
      "displayName": "Danilo Méndez",
      "userId": "17401835669463718921"
     },
     "user_tz": -120
    },
    "id": "1be38527-841e-405c-be81-bbb6b63cbd4f"
   },
   "outputs": [],
   "source": [
    "import tensorflow_ranking as tfr\n",
    "import tensorflow as tf\n",
    "from tensorflow_serving.apis import input_pb2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eec5900-63e0-4792-8723-95b38b8cd26e",
   "metadata": {
    "id": "9eec5900-63e0-4792-8723-95b38b8cd26e"
   },
   "source": [
    "## Import Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6da9c646-6efc-4578-adaa-1a026e748b81",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "executionInfo": {
     "elapsed": 5096,
     "status": "ok",
     "timestamp": 1652184036042,
     "user": {
      "displayName": "Danilo Méndez",
      "userId": "17401835669463718921"
     },
     "user_tz": -120
    },
    "id": "6da9c646-6efc-4578-adaa-1a026e748b81",
    "outputId": "04ff27a0-cdff-4425-a109-6d68e25d779a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAL</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>ABBV</th>\n",
       "      <th>ABC</th>\n",
       "      <th>ABT</th>\n",
       "      <th>ADP</th>\n",
       "      <th>AIG</th>\n",
       "      <th>AMD</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>AXP</th>\n",
       "      <th>...</th>\n",
       "      <th>UAL</th>\n",
       "      <th>UNH</th>\n",
       "      <th>UPS</th>\n",
       "      <th>USB</th>\n",
       "      <th>V</th>\n",
       "      <th>VZ</th>\n",
       "      <th>WFC</th>\n",
       "      <th>WMT</th>\n",
       "      <th>WY</th>\n",
       "      <th>XOM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-01-08</th>\n",
       "      <td>8.4760</td>\n",
       "      <td>13.162167</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.404000</td>\n",
       "      <td>-24.607200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.370000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>12.493714</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-9.7618</td>\n",
       "      <td>8.849167</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.031400</td>\n",
       "      <td>7.316500</td>\n",
       "      <td>33.678600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.957143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>11.965000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-4.760143</td>\n",
       "      <td>-3.706000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-11.584333</td>\n",
       "      <td>16.834600</td>\n",
       "      <td>-18.815571</td>\n",
       "      <td>-10.443167</td>\n",
       "      <td>...</td>\n",
       "      <td>12.935143</td>\n",
       "      <td>11.435500</td>\n",
       "      <td>-1.2800</td>\n",
       "      <td>-17.410857</td>\n",
       "      <td>-33.085800</td>\n",
       "      <td>-10.427667</td>\n",
       "      <td>21.078333</td>\n",
       "      <td>15.656600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-18.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-22</th>\n",
       "      <td>9.9768</td>\n",
       "      <td>-10.776667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-17.240429</td>\n",
       "      <td>-22.233833</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-10.898400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.001167</td>\n",
       "      <td>0.860833</td>\n",
       "      <td>...</td>\n",
       "      <td>2.058714</td>\n",
       "      <td>17.216800</td>\n",
       "      <td>-8.6162</td>\n",
       "      <td>-24.026800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-31.776000</td>\n",
       "      <td>-0.758500</td>\n",
       "      <td>-14.170714</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-19.522714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-29</th>\n",
       "      <td>-20.9400</td>\n",
       "      <td>-9.334000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.740000</td>\n",
       "      <td>-8.165800</td>\n",
       "      <td>-42.6148</td>\n",
       "      <td>-22.226000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-7.894167</td>\n",
       "      <td>-33.329167</td>\n",
       "      <td>...</td>\n",
       "      <td>9.101000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.8662</td>\n",
       "      <td>-52.884000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.221000</td>\n",
       "      <td>-20.201500</td>\n",
       "      <td>0.036800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.625571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-02-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-33.438857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.313000</td>\n",
       "      <td>-30.020400</td>\n",
       "      <td>-39.8710</td>\n",
       "      <td>-18.959800</td>\n",
       "      <td>-30.327667</td>\n",
       "      <td>-3.668714</td>\n",
       "      <td>-20.110167</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.529429</td>\n",
       "      <td>-36.037833</td>\n",
       "      <td>4.4648</td>\n",
       "      <td>-42.687167</td>\n",
       "      <td>14.249667</td>\n",
       "      <td>-47.767143</td>\n",
       "      <td>-7.948200</td>\n",
       "      <td>-45.108167</td>\n",
       "      <td>-28.786</td>\n",
       "      <td>8.348429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                AAL       AAPL  ABBV        ABC        ABT      ADP  \\\n",
       "Date                                                                  \n",
       "2012-01-08   8.4760  13.162167   NaN   0.404000 -24.607200      NaN   \n",
       "2012-01-15      NaN  11.965000   NaN  -4.760143  -3.706000      NaN   \n",
       "2012-01-22   9.9768 -10.776667   NaN -17.240429 -22.233833      NaN   \n",
       "2012-01-29 -20.9400  -9.334000   NaN  11.740000  -8.165800 -42.6148   \n",
       "2012-02-05      NaN -33.438857   NaN  -2.313000 -30.020400 -39.8710   \n",
       "\n",
       "                  AIG        AMD       AMZN        AXP  ...        UAL  \\\n",
       "Date                                                    ...              \n",
       "2012-01-08        NaN        NaN  -3.370000        NaN  ...  12.493714   \n",
       "2012-01-15 -11.584333  16.834600 -18.815571 -10.443167  ...  12.935143   \n",
       "2012-01-22 -10.898400        NaN  15.001167   0.860833  ...   2.058714   \n",
       "2012-01-29 -22.226000        NaN  -7.894167 -33.329167  ...   9.101000   \n",
       "2012-02-05 -18.959800 -30.327667  -3.668714 -20.110167  ...  -5.529429   \n",
       "\n",
       "                  UNH      UPS        USB          V         VZ        WFC  \\\n",
       "Date                                                                         \n",
       "2012-01-08        NaN  -9.7618   8.849167        NaN   8.031400   7.316500   \n",
       "2012-01-15  11.435500  -1.2800 -17.410857 -33.085800 -10.427667  21.078333   \n",
       "2012-01-22  17.216800  -8.6162 -24.026800        NaN -31.776000  -0.758500   \n",
       "2012-01-29        NaN  22.8662 -52.884000        NaN   8.221000 -20.201500   \n",
       "2012-02-05 -36.037833   4.4648 -42.687167  14.249667 -47.767143  -7.948200   \n",
       "\n",
       "                  WMT      WY        XOM  \n",
       "Date                                      \n",
       "2012-01-08  33.678600     NaN   1.957143  \n",
       "2012-01-15  15.656600     NaN -18.020000  \n",
       "2012-01-22 -14.170714     NaN -19.522714  \n",
       "2012-01-29   0.036800     NaN   6.625571  \n",
       "2012-02-05 -45.108167 -28.786   8.348429  \n",
       "\n",
       "[5 rows x 108 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables_names = ['big_log_ret','big_RCV', 'big_RVT', 'big_positivePartscr', 'big_negativePartscr', 'big_splogscr', 'big_linscr', 'big_linscr','big_lag1_log_ret','big_lag4_log_ret','big_lag1_month_log_ret']\n",
    "tables_dict = {}\n",
    "for name in tables_names:\n",
    "    table = pd.read_csv(os.path.join(PATH, 'Tables', name+'.csv'))\n",
    "    table['Date'] = pd.to_datetime(table['Date']).dt.date\n",
    "    table.set_index('Date', inplace=True)\n",
    "    tables_dict[name] = table\n",
    "\n",
    "tables_dict['big_RCV'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7eba07-f2c0-49fe-87a8-f57804e4c39e",
   "metadata": {
    "id": "dd7eba07-f2c0-49fe-87a8-f57804e4c39e"
   },
   "source": [
    "## Merging Tables - Building the query-stock final table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f47e7f6b-00b3-41b6-be1a-7528539882ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "executionInfo": {
     "elapsed": 651,
     "status": "ok",
     "timestamp": 1652184039643,
     "user": {
      "displayName": "Danilo Méndez",
      "userId": "17401835669463718921"
     },
     "user_tz": -120
    },
    "id": "f47e7f6b-00b3-41b6-be1a-7528539882ed",
    "outputId": "70a14784-5cea-4fb4-a6f6-1f2260733644"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>big_log_ret</th>\n",
       "      <th>big_RCV</th>\n",
       "      <th>big_RVT</th>\n",
       "      <th>big_positivePartscr</th>\n",
       "      <th>big_negativePartscr</th>\n",
       "      <th>big_splogscr</th>\n",
       "      <th>big_linscr</th>\n",
       "      <th>big_lag1_log_ret</th>\n",
       "      <th>big_lag4_log_ret</th>\n",
       "      <th>big_lag1_month_log_ret</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-01-08</td>\n",
       "      <td>AAL</td>\n",
       "      <td>0.063980</td>\n",
       "      <td>8.476000</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>0.020240</td>\n",
       "      <td>0.013340</td>\n",
       "      <td>0.010760</td>\n",
       "      <td>54.733580</td>\n",
       "      <td>0.099426</td>\n",
       "      <td>0.094986</td>\n",
       "      <td>-0.003565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-01-08</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.006150</td>\n",
       "      <td>13.162167</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.016050</td>\n",
       "      <td>0.015650</td>\n",
       "      <td>0.020333</td>\n",
       "      <td>48.380133</td>\n",
       "      <td>0.042066</td>\n",
       "      <td>-0.032534</td>\n",
       "      <td>0.070567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-01-08</td>\n",
       "      <td>ABC</td>\n",
       "      <td>-0.020683</td>\n",
       "      <td>0.404000</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>0.027820</td>\n",
       "      <td>0.012980</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>65.547120</td>\n",
       "      <td>0.036953</td>\n",
       "      <td>0.010394</td>\n",
       "      <td>0.059249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-01-08</td>\n",
       "      <td>ABT</td>\n",
       "      <td>0.000864</td>\n",
       "      <td>-24.607200</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.010820</td>\n",
       "      <td>0.018540</td>\n",
       "      <td>-0.012980</td>\n",
       "      <td>27.375300</td>\n",
       "      <td>-0.006602</td>\n",
       "      <td>0.005847</td>\n",
       "      <td>0.023364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-01-08</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>-0.023212</td>\n",
       "      <td>-3.370000</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.016180</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>-0.024640</td>\n",
       "      <td>40.688020</td>\n",
       "      <td>0.053483</td>\n",
       "      <td>-0.062913</td>\n",
       "      <td>-0.055493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42050</th>\n",
       "      <td>2021-11-28</td>\n",
       "      <td>UPS</td>\n",
       "      <td>-0.022512</td>\n",
       "      <td>-42.160333</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>0.018200</td>\n",
       "      <td>0.024717</td>\n",
       "      <td>-0.008817</td>\n",
       "      <td>46.677617</td>\n",
       "      <td>-0.015323</td>\n",
       "      <td>-0.024901</td>\n",
       "      <td>-0.034095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42051</th>\n",
       "      <td>2021-11-28</td>\n",
       "      <td>USB</td>\n",
       "      <td>-0.033782</td>\n",
       "      <td>-28.514833</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>0.020517</td>\n",
       "      <td>0.010367</td>\n",
       "      <td>0.037033</td>\n",
       "      <td>68.229333</td>\n",
       "      <td>-0.001564</td>\n",
       "      <td>0.005287</td>\n",
       "      <td>-0.048707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42052</th>\n",
       "      <td>2021-11-28</td>\n",
       "      <td>VZ</td>\n",
       "      <td>-0.007363</td>\n",
       "      <td>-22.016000</td>\n",
       "      <td>0.000686</td>\n",
       "      <td>0.018957</td>\n",
       "      <td>0.036629</td>\n",
       "      <td>-0.066443</td>\n",
       "      <td>28.958114</td>\n",
       "      <td>0.018313</td>\n",
       "      <td>-0.014255</td>\n",
       "      <td>-0.022713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42053</th>\n",
       "      <td>2021-11-28</td>\n",
       "      <td>WFC</td>\n",
       "      <td>-0.014140</td>\n",
       "      <td>-37.779000</td>\n",
       "      <td>0.000983</td>\n",
       "      <td>0.004283</td>\n",
       "      <td>0.017333</td>\n",
       "      <td>-0.061550</td>\n",
       "      <td>15.077883</td>\n",
       "      <td>-0.008021</td>\n",
       "      <td>-0.010320</td>\n",
       "      <td>-0.050985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42054</th>\n",
       "      <td>2021-11-28</td>\n",
       "      <td>WMT</td>\n",
       "      <td>-0.052347</td>\n",
       "      <td>-12.486143</td>\n",
       "      <td>0.000643</td>\n",
       "      <td>0.020143</td>\n",
       "      <td>0.027329</td>\n",
       "      <td>-0.029786</td>\n",
       "      <td>50.108929</td>\n",
       "      <td>0.017474</td>\n",
       "      <td>0.005406</td>\n",
       "      <td>-0.030717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42055 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Query Ticker  big_log_ret    big_RCV   big_RVT  \\\n",
       "0      2012-01-08    AAL     0.063980   8.476000  0.000280   \n",
       "1      2012-01-08   AAPL    -0.006150  13.162167  0.003400   \n",
       "2      2012-01-08    ABC    -0.020683   0.404000  0.000760   \n",
       "3      2012-01-08    ABT     0.000864 -24.607200  0.000320   \n",
       "4      2012-01-08   AMZN    -0.023212  -3.370000  0.001100   \n",
       "...           ...    ...          ...        ...       ...   \n",
       "42050  2021-11-28    UPS    -0.022512 -42.160333  0.000317   \n",
       "42051  2021-11-28    USB    -0.033782 -28.514833  0.000483   \n",
       "42052  2021-11-28     VZ    -0.007363 -22.016000  0.000686   \n",
       "42053  2021-11-28    WFC    -0.014140 -37.779000  0.000983   \n",
       "42054  2021-11-28    WMT    -0.052347 -12.486143  0.000643   \n",
       "\n",
       "       big_positivePartscr  big_negativePartscr  big_splogscr  big_linscr  \\\n",
       "0                 0.020240             0.013340      0.010760   54.733580   \n",
       "1                 0.016050             0.015650      0.020333   48.380133   \n",
       "2                 0.027820             0.012980      0.017000   65.547120   \n",
       "3                 0.010820             0.018540     -0.012980   27.375300   \n",
       "4                 0.016180             0.020000     -0.024640   40.688020   \n",
       "...                    ...                  ...           ...         ...   \n",
       "42050             0.018200             0.024717     -0.008817   46.677617   \n",
       "42051             0.020517             0.010367      0.037033   68.229333   \n",
       "42052             0.018957             0.036629     -0.066443   28.958114   \n",
       "42053             0.004283             0.017333     -0.061550   15.077883   \n",
       "42054             0.020143             0.027329     -0.029786   50.108929   \n",
       "\n",
       "       big_lag1_log_ret  big_lag4_log_ret  big_lag1_month_log_ret  \n",
       "0              0.099426          0.094986               -0.003565  \n",
       "1              0.042066         -0.032534                0.070567  \n",
       "2              0.036953          0.010394                0.059249  \n",
       "3             -0.006602          0.005847                0.023364  \n",
       "4              0.053483         -0.062913               -0.055493  \n",
       "...                 ...               ...                     ...  \n",
       "42050         -0.015323         -0.024901               -0.034095  \n",
       "42051         -0.001564          0.005287               -0.048707  \n",
       "42052          0.018313         -0.014255               -0.022713  \n",
       "42053         -0.008021         -0.010320               -0.050985  \n",
       "42054          0.017474          0.005406               -0.030717  \n",
       "\n",
       "[42055 rows x 12 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_table = tables_dict['big_log_ret'].stack() #Stack DF to a Series so that is grouped by Date and then by Ticker\n",
    "final_table.index.names=('Query','Ticker') #Rename 'Date' to 'Query'\n",
    "final_table = final_table.reset_index()\n",
    "final_table.rename(columns={0: 'big_log_ret'}, inplace=True)\n",
    "resting_tables_dict = dict(tables_dict) #Make a copy of the dictionary\n",
    "del resting_tables_dict['big_log_ret'] #Remove the log_ret table\n",
    "\n",
    "for i, t_name in enumerate(resting_tables_dict):\n",
    "    table = tables_dict[t_name].stack()\n",
    "    table.index.names=('Query','Ticker')\n",
    "    table = table.reset_index()\n",
    "    table.rename(columns={0: t_name}, inplace=True)\n",
    "    final_table = pd.merge(final_table, table, on=['Query','Ticker'])\n",
    "\n",
    "final_table.set_index('Query', inplace=False).to_csv(os.path.join(PATH,'Tables','final_table.csv')) #Store in csv format in the 'Tables' folder\n",
    "final_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d1eb957c-0e1e-4295-861b-05c2ade20008",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 779,
     "status": "ok",
     "timestamp": 1652179470883,
     "user": {
      "displayName": "Danilo Méndez",
      "userId": "17401835669463718921"
     },
     "user_tz": -120
    },
    "id": "d1eb957c-0e1e-4295-861b-05c2ade20008",
    "outputId": "3e601661-afb4-414e-e6af-8470e3f6694f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Aqui tenemos q añadir un filtro para eliminar aquellas filas q tengan un NaN y tambien si queremos hacer algun preprocessing previo\n",
    "'''\n",
    "final_table.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "H6oFUdFB8lsy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 235,
     "status": "ok",
     "timestamp": 1652169114617,
     "user": {
      "displayName": "Danilo Méndez",
      "userId": "17401835669463718921"
     },
     "user_tz": -120
    },
    "id": "H6oFUdFB8lsy",
    "outputId": "2f3b0e96-ee9b-489e-86ce-c8055e4dc78b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Es posible que necesitemos pasar todos los log returns a no negativos sumandole el valor absoluto del minimo a toda la columna para que sean positivos\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8wPIbh6RkiT9",
   "metadata": {
    "id": "8wPIbh6RkiT9"
   },
   "source": [
    "## Data Splitting\n",
    "\n",
    "We are looking to split the data to train our model, the problem with time series split train tests is that we use 95% to train the data and test it on the last 5%. There is a possibility that our test data behaves nicely or we just got lucky. To combat this event we do an expanding window test. Where we use some of the data to predict the next period, then we expand the data and repeat. This way we can have a more testing sets with the same data in an appropiate way.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "257bfcab-a171-445c-8d7e-1c8df8f8b417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expanding_window_split(dataframe, number_of_splits:int):\n",
    "    '''\n",
    "    Splits the dataframe into 'number of splits' to train and test sets to predict one period ahead. It produces the number of splits starting from the\n",
    "    last element onwards in a decreasing manner. \n",
    "    '''\n",
    "    all_splits = {}\n",
    "    for i in range(number_of_splits):\n",
    "        test_query = dataframe['Query'].unique()[-(i+1)]\n",
    "        train_query = dataframe['Query'].unique()[:-(i+1)]\n",
    "        test = dataframe[dataframe['Query'].isin([test_query])]\n",
    "        train = dataframe[dataframe['Query'].isin(train_query)]\n",
    "        all_splits[\"split_\"+str(number_of_splits-i)] = (train, test)\n",
    "    return all_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "8ca0e56b-6d92-4733-bbac-844b4da3c95b",
   "metadata": {
    "id": "8ca0e56b-6d92-4733-bbac-844b4da3c95b"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Protobuffers are extensible structures suitable for storing data in a serialized format, either locally or in a distributed manner.\n",
    "TF ranking has a couple of pre-defined protobufs such as ELWC which make it easier to integrate and formalize data ingestion into\n",
    "the ranking pipeline.\n",
    "\n",
    "Protocol buffers and the tf.data API is a set of utilities that provide a mechanism to read and store data for efficient loading\n",
    "and preprocessing in a way that's fast and scalable.\n",
    "'''\n",
    "\n",
    "# Class I wrote to organize code resposible for parsing our dataframe data and \n",
    "# creating compressed TF-Records in ELWC protobuf format so that they used by \n",
    "# most rankers in TF-Ranking and be compatible with future rankers or new methods\n",
    "\n",
    "class DFToELWCProto():\n",
    "    \"\"\" Class to parse Dataframe ranking data in ELWC proto TFRecords\"\"\"\n",
    "    \n",
    "    def __init__(self, dir:str=\".\", use_compression:bool=False):\n",
    "        assert isinstance(dir,str)\n",
    "        if not os.path.isdir(dir):\n",
    "            os.mkdir(dir)\n",
    "        self.input_path = dir\n",
    "        assert isinstance(use_compression,bool)\n",
    "        self.compress = use_compression\n",
    "        if self.compress:\n",
    "            self.compress_type = 'GZIP'\n",
    "        else:\n",
    "            self.compress_type = None\n",
    "\n",
    "\n",
    "    # Helper functions (see also https://www.tensorflow.org/tutorials/load_data/tf_records)\n",
    "    def _bytes_feature(self,value_list):\n",
    "        \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "        if isinstance(value_list, type(tf.constant(0))):\n",
    "            value_list = value_list.numpy()\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value_list]))\n",
    "\n",
    "\n",
    "    def _float_feature(self,value_list):\n",
    "        \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "        return tf.train.Feature(float_list=tf.train.FloatList(value=[value_list]))\n",
    "\n",
    "    def _int64_feature(self,value_list):\n",
    "        \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value_list]))\n",
    "\n",
    "\n",
    "    def print_topn_tfrecord(self, target_filename, num_of_examples_to_print=-1):\n",
    "        filenames = [target_filename]\n",
    "        tf_record_dataset = tf.data.TFRecordDataset(filenames,\n",
    "                                                    compression_type=self.compress_type)\n",
    "\n",
    "        for raw_record in tf_record_dataset.take(num_of_examples_to_print):\n",
    "            example_list_with_context = input_pb2.ExampleListWithContext()\n",
    "            example_list_with_context.ParseFromString(raw_record.numpy())\n",
    "            print(example_list_with_context)\n",
    "    \n",
    "    def read_topn_tfrecord(self, target_filename, num_of_examples_to_read=-1):\n",
    "        filenames = [target_filename]\n",
    "        tf_record_dataset = tf.data.TFRecordDataset(filenames,\n",
    "                                                    compression_type=self.compress_type)\n",
    "        queries_with_context_and_exmples=[]\n",
    "        for raw_record in tf_record_dataset.take(num_of_examples_to_read):\n",
    "            example_list_with_context = input_pb2.ExampleListWithContext()\n",
    "            example_list_with_context.ParseFromString(raw_record.numpy())\n",
    "            queries_with_context_and_exmples.append(example_list_with_context)\n",
    "        return queries_with_context_and_exmples\n",
    "\n",
    "    def df_to_TFrecord(self, df, file_name):\n",
    "\n",
    "        \"\"\" \n",
    "        for reading and converting directly from arrays in memory\n",
    "        \n",
    "        \"\"\"    \n",
    "        file_name = os.path.basename(file_name)\n",
    "\n",
    "        if self.compress:\n",
    "            print('Using GZIP compression for writing ELWC TFRecord Dataset')\n",
    "            opts = tf.io.TFRecordOptions(compression_type = self.compress_type)\n",
    "            file_name = f\"{file_name}.gzipped_tfrecord\"\n",
    "        else:\n",
    "            opts = tf.io.TFRecordOptions(compression_type = self.compress_type)\n",
    "            file_name = f\"{file_name}.tfrecord\"\n",
    "        save_path = f\"{self.input_path}/{file_name}\"\n",
    "\n",
    "        with tf.io.TFRecordWriter(f\"{save_path}\", options=opts) as writer:\n",
    "            \n",
    "            input_array = np.array(df)\n",
    "            col_names = df.columns\n",
    "\n",
    "            ELWC = input_pb2.ExampleListWithContext()\n",
    "            prev_qid = None\n",
    "\n",
    "            for i in range(input_array.shape[0]):\n",
    "\n",
    "                qid, doc, r, features = str(input_array[i,0]), input_array[i,1], input_array[i,2], input_array[i,3:]\n",
    "\n",
    "                example_proto_dict = {\n",
    "                              f\"{col_names[f_n+3]}\":self._float_feature((f_v))\n",
    "                                  for (f_n, f_v) in enumerate(features)\n",
    "                          }\n",
    "                example_proto_dict['rel'] = self._float_feature(r)\n",
    "                example_proto_dict['doc'] = self._bytes_feature(doc.encode('utf-8'))\n",
    "\n",
    "                example_proto = tf.train.Example(features=tf.train.Features(feature=example_proto_dict))\n",
    "                if qid != prev_qid:\n",
    "                    if prev_qid is not None:\n",
    "                        writer.write(ELWC.SerializeToString())\n",
    "                    prev_qid = qid\n",
    "                    \n",
    "                    context_proto_dict = {\n",
    "                        \"qid\" : self._bytes_feature(\"{}\".format(qid).encode('utf-8'))\n",
    "                        }\n",
    "                    context_proto = tf.train.Example(features=tf.train.Features(feature=context_proto_dict))\n",
    "                    \n",
    "                    ELWC = input_pb2.ExampleListWithContext()\n",
    "                    ELWC.context.CopyFrom(context_proto)\n",
    "                    ELWC.examples.append(example_proto)\n",
    "                else:\n",
    "                    ELWC.examples.append(example_proto)\n",
    "\n",
    "            # final write for the last query grp\n",
    "            writer.write(ELWC.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "80deaad5-af6a-492a-b172-05304d3873d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Get the unique Queries and do the split based on that\n",
    "query_train, query_test = train_test_split(final_table.Query.unique(), test_size=0.3, random_state=42, shuffle=False)\n",
    "train = final_table[final_table.Query.isin(query_train)]\n",
    "test = final_table[final_table.Query.isin(query_test)]\n",
    "\n",
    "#Convert Train and Test dataset to ELWC and store it\n",
    "ELWC_converter = DFToELWCProto(dir = \"./Dataset-tfrecords/70v30Split\", use_compression=False)\n",
    "ELWC_converter.df_to_TFrecord(train,\"train\")\n",
    "ELWC_converter.df_to_TFrecord(test,\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "39115f8d-8766-4382-b8e7-0dd11c588064",
   "metadata": {
    "id": "39115f8d-8766-4382-b8e7-0dd11c588064",
    "outputId": "fe9ba8ff-8849-4daf-a2ad-ec6370a7a128"
   },
   "outputs": [],
   "source": [
    "#Number of splits that we will do to our dataframe \n",
    "_NUM_SPLITS = 48\n",
    "\n",
    "#Create a dictionary of tuples containing all the splits we will use on our LTR model. Each tuple contains two dataframes (train, test)\n",
    "splits_dict = expanding_window_split(final_table, _NUM_SPLITS)\n",
    "\n",
    "#Convert all those dataframes into ELWC\n",
    "for i in range(_NUM_SPLITS):\n",
    "    split_name = \"split_{}\".format(i+1)\n",
    "    train, test = splits_dict[split_name][0], splits_dict[split_name][1]\n",
    "    \n",
    "    #Convert Train dataset to ELWC and store it\n",
    "    ELWC_converter_train = DFToELWCProto(dir = \"./Dataset-tfrecords/Split_{}\".format(i+1), use_compression=False)\n",
    "    ELWC_converter_train.df_to_TFrecord(train,\"train\")\n",
    "    \n",
    "    #Convert Test dataset to ELWC and store it\n",
    "    ELWC_converter_test = DFToELWCProto(dir = \"./Dataset-tfrecords/Split_{}\".format(i+1), use_compression=False)\n",
    "    ELWC_converter_test.df_to_TFrecord(test,\"test\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Input_Pipeline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
